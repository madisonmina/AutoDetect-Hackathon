{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6930eedb-f036-4e46-8b3a-e551905007e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-29 14:11:01.933884: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-11-29 14:11:01.950666: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-11-29 14:11:01.969989: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8473] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-11-29 14:11:01.976364: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1471] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-11-29 14:11:01.991762: I tensorflow/core/platform/cpu_feature_guard.cc:211] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE3 SSE4.1 SSE4.2 AVX, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.applications import MobileNetV2\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a550302e-af17-47fa-b2e0-ccaf384f2b08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading from https://www.kaggle.com/api/v1/datasets/download/ravirajsinh45/real-life-industrial-dataset-of-casting-product?dataset_version_number=2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100M/100M [00:09<00:00, 11.6MB/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting files...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import kagglehub\n",
    "\n",
    "# Download latest version\n",
    "path = kagglehub.dataset_download(\"ravirajsinh45/real-life-industrial-dataset-of-casting-product\")\n",
    "\n",
    "# print(\"Path to dataset files:\", path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ee499a91-fffd-4aad-8ce3-adadfb6fe495",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = path + r'/casting_data/casting_data/train'\n",
    "testset = path + r'/casting_data/casting_data/test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "27565673-3c58-4f72-b3e9-f58b6b691d25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instead of trying to save the model, create a class that takes inputs from the user and provides outputs\n",
    "\n",
    "class DefectDetector:\n",
    "    IMG_SIZE = 244\n",
    "    def __init__(self, train_path):\n",
    "        '''\n",
    "        Given a training image, create a model that will predict \n",
    "        defects. Training data is expected to contain two folders,\n",
    "        one for defect and the other for no defect\n",
    "        '''\n",
    "        train_datagen = ImageDataGenerator(\n",
    "            rescale=1./255,\n",
    "            rotation_range=20,\n",
    "            width_shift_range=0.1,\n",
    "            height_shift_range=0.1,\n",
    "            horizontal_flip=True,\n",
    "            validation_split=0.1 \n",
    "        )\n",
    "\n",
    "        train_data = train_datagen.flow_from_directory(\n",
    "            train_path,\n",
    "            target_size=(self.IMG_SIZE, self.IMG_SIZE), \n",
    "            batch_size=1,\n",
    "            class_mode='sparse',\n",
    "            subset='training',\n",
    "            shuffle=False\n",
    "        )\n",
    "\n",
    "        num_classes = len(train_data.class_indices)\n",
    "        self.class_indices = train_data.class_indices\n",
    "\n",
    "        self._create_model(num_classes)\n",
    "        self._train_model(train_data)\n",
    "        \n",
    "\n",
    "    # Preprocessing\n",
    "    def preprocess_defect_image(self, path):\n",
    "        # 1. Load image\n",
    "        img = cv2.imread(path, cv2.IMREAD_COLOR)\n",
    "        if img is None:\n",
    "            raise ValueError(f\"Cannot load image: {path}\")\n",
    "    \n",
    "        # 2. Convert to grayscale\n",
    "        gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "        # Skip in the interest of simplifying\n",
    "        # 3. Apply CLAHE (contrast enhancement)\n",
    "        # clahe = cv2.createCLAHE(clipLimit=3.0, tileGridSize=(8, 8))\n",
    "        # enhanced = clahe.apply(gray)\n",
    "    \n",
    "        # 4. Sharpen to highlight cracks/dents\n",
    "        kernel = np.array([\n",
    "            [0, -1,  0],\n",
    "            [-1, 5, -1],\n",
    "            [0, -1,  0]\n",
    "        ])\n",
    "        sharpened = cv2.filter2D(gray, -1, kernel)\n",
    "    \n",
    "        # 5. Convert back to 3-channel BGR (CNNs expect 3 channels)\n",
    "        output = cv2.cvtColor(sharpened, cv2.COLOR_GRAY2BGR)\n",
    "    \n",
    "        return output\n",
    "\n",
    "    def extract_embeddings_from_generator(self, generator):\n",
    "        X = []\n",
    "        y = []\n",
    "    \n",
    "        file_paths = generator.filepaths\n",
    "        labels = generator.classes\n",
    "    \n",
    "        for idx, path in enumerate(file_paths):\n",
    "            # custom preprocessing\n",
    "            processed = self.preprocess_defect_image(path)\n",
    "            processed = self.preprocess_for_model(processed)\n",
    "    \n",
    "            X.append(processed)\n",
    "            y.append(labels[idx])\n",
    "    \n",
    "        X = np.array(X)\n",
    "        y = np.array(y)\n",
    "    \n",
    "        # extract MobileNet embeddings\n",
    "        embeddings = self.feature_extractor.predict(X, batch_size=32)\n",
    "        return embeddings, y\n",
    "\n",
    "    # Preprocess, resize, normailze\n",
    "    def preprocess_for_model(self, img):\n",
    "        \"\"\"Takes a BGR OpenCV image and returns MobileNetV2-ready array.\"\"\"\n",
    "        img = cv2.resize(img, (224, 224))\n",
    "        img = tf.keras.applications.mobilenet_v2.preprocess_input(img.astype(np.float32))\n",
    "        return img\n",
    "    \n",
    "    def _create_model(self, num_classes):\n",
    "        self.feature_extractor = MobileNetV2(\n",
    "            input_shape=(224, 224, 3),\n",
    "            pooling='avg',\n",
    "            include_top=False,\n",
    "            weights='imagenet'\n",
    "        )\n",
    "        self.feature_extractor.trainable = False  # freeze for fast training\n",
    "\n",
    "        self.classifier = models.Sequential([\n",
    "            layers.Input(shape=(1280,)),\n",
    "            layers.Dense(256, activation=\"relu\"),\n",
    "            layers.Dropout(0.3),\n",
    "            layers.Dense(num_classes, activation=\"softmax\")\n",
    "        ])\n",
    "        self.classifier.compile(\n",
    "            optimizer=\"adam\",\n",
    "            loss=\"sparse_categorical_crossentropy\",\n",
    "            metrics=[\"accuracy\"]\n",
    "        )\n",
    "\n",
    "    def _train_model(self, train_data):\n",
    "        train_embeddings, train_labels = self.extract_embeddings_from_generator(train_data)\n",
    "\n",
    "        num_classes = len(train_data.class_indices)\n",
    "\n",
    "        self.classifier.fit(\n",
    "            train_embeddings, train_labels,\n",
    "            batch_size=32,\n",
    "            epochs=10,\n",
    "            validation_split=0.1\n",
    "        )\n",
    "        \n",
    "    def evaluate(self, test_path):\n",
    "        test_datagen = ImageDataGenerator(\n",
    "            rescale=1./255,\n",
    "            rotation_range=20,\n",
    "            width_shift_range=0.1,\n",
    "            height_shift_range=0.1,\n",
    "            horizontal_flip=True\n",
    "        )\n",
    "        \n",
    "        test_data = test_datagen.flow_from_directory(\n",
    "            test_path,\n",
    "            target_size=(self.IMG_SIZE, self.IMG_SIZE),\n",
    "            batch_size=1,\n",
    "            class_mode='sparse',\n",
    "            shuffle=False\n",
    "        )\n",
    "        \n",
    "        test_embeddings, test_labels = self.extract_embeddings_from_generator(test_data)\n",
    "        loss, acc = self.classifier.evaluate(test_embeddings, test_labels)\n",
    "        print(\"Test accuracy:\", acc)\n",
    "\n",
    "    def predict(self, image_path):\n",
    "        '''\n",
    "        predict and return predicted class and confidence\n",
    "        '''\n",
    "        processed = self.preprocess_defect_image(image_path)\n",
    "        processed = self.preprocess_for_model(processed)\n",
    "        processed = np.expand_dims(processed, axis=0)\n",
    "        embedding = self.feature_extractor.predict(processed, verbose=0)\n",
    "        pred_probs = self.classifier.predict(embedding, verbose=0)\n",
    "        \n",
    "        idx_to_class = {v: k for k, v in self.class_indices.items()}\n",
    "        class_idx = np.argmax(pred_probs)\n",
    "        pred_class = idx_to_class[class_idx]\n",
    "        \n",
    "        return pred_class, pred_probs[0][class_idx]\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "179baed6-df25-4fed-8b7c-fe22b2790912",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5971 images belonging to 2 classes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-29 14:11:29.272517: E external/local_xla/xla/stream_executor/cuda/cuda_driver.cc:266] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "187/187 [==============================] - 22s 112ms/step\n",
      "Epoch 1/10\n",
      "168/168 [==============================] - 1s 3ms/step - loss: 0.2577 - accuracy: 0.8935 - val_loss: 0.0580 - val_accuracy: 0.9916\n",
      "Epoch 2/10\n",
      "168/168 [==============================] - 1s 3ms/step - loss: 0.0825 - accuracy: 0.9726 - val_loss: 0.0486 - val_accuracy: 0.9900\n",
      "Epoch 3/10\n",
      "168/168 [==============================] - 1s 4ms/step - loss: 0.0536 - accuracy: 0.9825 - val_loss: 0.0174 - val_accuracy: 1.0000\n",
      "Epoch 4/10\n",
      "168/168 [==============================] - 1s 4ms/step - loss: 0.0467 - accuracy: 0.9844 - val_loss: 0.0054 - val_accuracy: 1.0000\n",
      "Epoch 5/10\n",
      "168/168 [==============================] - 1s 4ms/step - loss: 0.0355 - accuracy: 0.9896 - val_loss: 0.0155 - val_accuracy: 0.9983\n",
      "Epoch 6/10\n",
      "168/168 [==============================] - 1s 4ms/step - loss: 0.0379 - accuracy: 0.9888 - val_loss: 0.0167 - val_accuracy: 0.9967\n",
      "Epoch 7/10\n",
      "168/168 [==============================] - 1s 4ms/step - loss: 0.0373 - accuracy: 0.9879 - val_loss: 0.0838 - val_accuracy: 0.9749\n",
      "Epoch 8/10\n",
      "168/168 [==============================] - 1s 4ms/step - loss: 0.0370 - accuracy: 0.9873 - val_loss: 0.0024 - val_accuracy: 1.0000\n",
      "Epoch 9/10\n",
      "168/168 [==============================] - 1s 4ms/step - loss: 0.0323 - accuracy: 0.9894 - val_loss: 0.0860 - val_accuracy: 0.9682\n",
      "Epoch 10/10\n",
      "168/168 [==============================] - 1s 4ms/step - loss: 0.0295 - accuracy: 0.9907 - val_loss: 0.0282 - val_accuracy: 0.9916\n",
      "CPU times: user 2min 56s, sys: 11 s, total: 3min 7s\n",
      "Wall time: 51.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "detector = DefectDetector(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f097d5b3-5b17-428d-a5a0-9959fc8091e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 715 images belonging to 2 classes.\n",
      "23/23 [==============================] - 3s 114ms/step\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0281 - accuracy: 0.9944\n",
      "Test accuracy: 0.9944055676460266\n"
     ]
    }
   ],
   "source": [
    "detector.evaluate(testset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a64efb30-1d86-487b-b9e2-7173efc40873",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing prediction\n",
    "def list_files(path):\n",
    "    return [\n",
    "        os.path.join(path, f)\n",
    "        for f in os.listdir(path)\n",
    "        if os.path.isfile(os.path.join(path, f))\n",
    "    ]\n",
    "\n",
    "defect = path + r'/casting_data/casting_data/test/def_front'\n",
    "ok = path + r'/casting_data/casting_data/test/ok_front'\n",
    "\n",
    "defective = list_files(defect)\n",
    "fine = list_files(ok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "141f22a0-e04b-4dbb-9ba8-73c2c8725338",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.004415011037527594"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fail_def = 0\n",
    "total_def = len(defective)\n",
    "\n",
    "for img in defective:\n",
    "    pred_class, conf = detector.predict(img)\n",
    "    if pred_class != \"def_front\":\n",
    "        fail_def += 1\n",
    "\n",
    "fail_def / total_def"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fbbd666f-20cf-44cd-988c-9ab5490f720c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.007633587786259542"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fail_ok = 0\n",
    "total_ok = len(fine)\n",
    "\n",
    "for img in fine:\n",
    "    pred_class, conf = detector.predict(img)\n",
    "    if pred_class != \"ok_front\":\n",
    "        fail_ok += 1\n",
    "\n",
    "fail_ok / total_ok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "805ceeff-3b25-4fea-baf0-9a19484c9d72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 133 ms, sys: 9.66 ms, total: 143 ms\n",
      "Wall time: 113 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('def_front', 0.999987)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "detector.predict(defective[9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bd50239b-c529-43a0-8497-0b6b3dfc18e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6633"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "defect = path + r'/casting_data/casting_data/train/def_front'\n",
    "ok = path + r'/casting_data/casting_data/train/ok_front'\n",
    "\n",
    "defective = list_files(defect)\n",
    "fine = list_files(ok)\n",
    "len(defective) + len(fine)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06a32b8a-9984-4e41-955b-2636188c978b",
   "metadata": {},
   "source": [
    "## Notes\n",
    "\n",
    "- The model uses ImageNet to extract features from training images and uses a much simpler classifier model in order to differentiate between defect or no defect.\n",
    "- The model takes no more than 5 minutes to train and a couple of seconds to predict.\n",
    "- Provided a good amount of training images, the model achieves a high accuracy (above 90%). In the above training, there are 6,633 labeled images for training. Obviously less training images leads to less accuracy. This drop however is somewhat reasonable, with the test accuracy with 40 images (20 defect + 20 ok) being above 70% in a majority of cases. The model variance seems to be pretty high with the accuracy going from ~63% to ~84%. If customers continue to label images, we can feed this back in to the model and improve the accuracy. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
